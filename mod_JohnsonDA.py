""" 
Functions needed for Johnson 3DVAR

#### Methods:
- `sol_L63`        ->   Solution to the Lorenz-63 model
- `gen_obs`        ->   Generation of observations from true state
- `johnson_var3d`  ->   Johnson 3DVAR algorithm

#### Author:
Senne Van Loon
Cooperative Institute for Research in the Atmosphere (CIRA),
Colorado State University,
3925A West Laporte Ave, Fort Collins, CO 80521

#### References and acknowledgements:
* Johnson, N. L. (1949). Systems of Frequency Curves Generated by Methods of Translation. Biometrika, 36(1/2), 149-176.
* Lorenz, E. N. (1963). Deterministic nonperiodic flow. Journal of atmospheric sciences, 20(2), 130-141.
* Senne Van Loon and Steven J. Fletcher (2023). Foundations for Universal Non-Gaussian Data Assimilation, Geophysical Research Letters

"""

# Load modules
import numpy as np
from scipy.linalg import inv
from scipy.optimize import minimize

from ctypes import c_double, c_int, CDLL

# Load C-library
import os 
dir_path = os.path.dirname(os.path.realpath(__file__))
c_mod = CDLL(dir_path+"/C_Lorenz.so")

import warnings
warnings.filterwarnings("error", category=RuntimeWarning)


##################################################################################
################################    Lorenz-63     ################################
##################################################################################




def sol_L63(t, x0, p = np.array([10.0,28.0,8.0/3.0])):
    N = 3
    if x0.size != N:
        raise RuntimeError("Incorrect initial conditions!")

    t0 = c_double(t[0])
    dt = c_double(t[1]-t[0])
    nt = t.size

    x0_c = (c_double * N)(*x0)
    sol_c = (c_double * (N*nt))()

    s = c_double(p[0])
    r = c_double(p[1])
    b = c_double(p[2])

    c_mod.sol_L63_(t0,dt,c_int(nt),x0_c,sol_c,s,r,b)

    return np.reshape(np.array(sol_c[:]),(N,nt),order='F')




##################################################################################
############################    Johnson functions    #############################
##################################################################################


# Johnson transforms
def t_normal(x,xi,lam):
    return (x-xi)/lam
def t_semibounded(x,xi,lam):
    return np.log((x-xi)/lam)
def t_bounded(x,xi,lam):
    return np.log((x-xi)/(lam+xi-x))
def t_unbounded(x,xi,lam):
    return np.arcsinh((x-xi)/lam)

# Inverse Johnson transforms
def inv_t_normal(t,xi,lam):
    return xi + lam*t
def inv_t_semibounded(t,xi,lam):
    return xi + lam*np.exp(t)
def inv_t_bounded(t,xi,lam):
    return (xi + (lam+xi)*np.exp(t))/(1.0+np.exp(t))
def inv_t_unbounded(t,xi,lam):
    return xi + lam*np.sinh(t)

# Derivatives of Johnson transforms, dT/dx(T^-1(t))
def s_normal(t):
    return np.ones_like(t)
def s_semibounded(t):
    return np.exp(-t)
def s_bounded(t):
    return 2.0 + 2.0*np.cosh(t)
def s_unbounded(t):
    return 1.0/np.cosh(t)

# Log of derivatives of Johnson transforms, log dT/dx(T^-1(t))
def log_s_normal(t):
    return np.zeros_like(t)
def log_s_semibounded(t):
    return -t
def log_s_bounded(t):
    return np.log(2.0 + 2.0*np.cosh(t))
def log_s_unbounded(t):
    return -np.log(np.cosh(t))

# Second derivatives of Johnson transforms, d[log S(t)]/dt
def f_normal(t):
    return np.zeros_like(t)
def f_semibounded(t):
    return -np.ones_like(t)
def f_bounded(t):
    return np.tanh(t/2.0)
def f_unbounded(t):
    return -np.tanh(t)

# Combinations for simple function selection
def t_fun_L63(x,xi,lam,str_transform,N):
    t = np.copy(x)

    for jj in range(N):
        if   str_transform[jj]=='normal':
            t[jj] = t_normal(x[jj],xi[jj],lam[jj])
        elif str_transform[jj]=='semibounded':
            t[jj] = t_semibounded(x[jj],xi[jj],lam[jj])
        elif str_transform[jj]=='bounded':
            t[jj] = t_bounded(x[jj],xi[jj],lam[jj])
        elif str_transform[jj]=='unbounded':
            t[jj] = t_unbounded(x[jj],xi[jj],lam[jj])
        else:
            raise ValueError("Transform name "+str_transform[jj]+" is not valid.")
    return t

def inv_t_fun_L63(t,xi,lam,str_transform,N):
    x = np.copy(t)

    for jj in range(N):
        if   str_transform[jj]=='normal':
            x[jj] = inv_t_normal(t[jj],xi[jj],lam[jj])
        elif str_transform[jj]=='semibounded':
            x[jj] = inv_t_semibounded(t[jj],xi[jj],lam[jj])
        elif str_transform[jj]=='bounded':
            x[jj] = inv_t_bounded(t[jj],xi[jj],lam[jj])
        elif str_transform[jj]=='unbounded':
            x[jj] = inv_t_unbounded(t[jj],xi[jj],lam[jj])
        else:
            raise ValueError("Transform name "+str_transform[jj]+" is not valid.")
    return x

def log_s_fun_L63(t,str_transform,N):
    x = np.copy(t)

    for jj in range(N):
        if   str_transform[jj]=='normal':
            x[jj] = log_s_normal(t[jj])
        elif str_transform[jj]=='semibounded':
            x[jj] = log_s_semibounded(t[jj])
        elif str_transform[jj]=='bounded':
            x[jj] = log_s_bounded(t[jj])
        elif str_transform[jj]=='unbounded':
            x[jj] = log_s_unbounded(t[jj])
        else:
            raise ValueError("Transform name "+str_transform[jj]+" is not valid.")
    return x

def f_fun_L63(t,str_transform,N):
    x = np.copy(t)

    for jj in range(N):
        if   str_transform[jj]=='normal':
            x[jj] = f_normal(t[jj])
        elif str_transform[jj]=='semibounded':
            x[jj] = f_semibounded(t[jj])
        elif str_transform[jj]=='bounded':
            x[jj] = f_bounded(t[jj])
        elif str_transform[jj]=='unbounded':
            x[jj] = f_unbounded(t[jj])
        else:
            raise ValueError("Transform name "+str_transform[jj]+" is not valid.")
    return x


##################################################################################
################################    DA Setup    ##################################
##################################################################################


def gen_obs(t, SV, period_obs, H, var_obs, transform, inv_transform, \
            seed = None):
    """
    Generate noisy observations from true state nature run

    #### Input
    - `t`               ->  Time of truth, vector of length n_t
    - `SV`              ->  State variables of truth, array of size n_SV x n_t,
                            with n_SV the number of variables, and n_t the number of time steps
    - `period_obs`      ->  Observation period, in steps of truth time
    - `H`               ->  Observation operator, function of the form y = h(SV)
    - `var_obs`         ->  Observational error variance, diagonal values of 
                            observational error covariance matrix R
    - `transform`       ->  Transform function, of the form X = T(x)
    - `inv_transform`   ->  Inverse transform function, of the form x = T^-1(X)
    - `seed`            ->  Seed of the random number generator. Default is random seed

    #### Output 
    - `t_obs`           ->  Time of observations, vector of length n_t_obs = n_t//period_obs
    - `y`               ->  Observations, array of size n_obs x n_t_obs
    - `R`               ->  Observational error covariance matrix of size n_obs x n_obs
    """

    # Create time array of the observations
    t_obs = t[::period_obs]

    # State variables at observation times
    SV_obs = SV[:,::period_obs]

    # Initialize observations array
    y = H(SV_obs)
    n_obs = y.shape[0]

    # Create observational error covariance matrix 
    R = var_obs*np.eye(n_obs)

    # Create random errors
    rng = np.random.default_rng(seed)
    err = rng.normal(0.0, np.sqrt(var_obs), size=y.shape)

    # Transform observed true state
    y = transform(y)

    # Add noise to observations
    y = inv_transform(y + err)

    return t_obs, y, R
    

##################################################################################
##################################    3DVAR    ###################################
##################################################################################

def johnson_var3d(init_guess, t_obs, n_t_mod, y, phi, dphi, B, R, model, \
    t_fun_SV, t_fun_obs, inv_t_fun_SV, \
    log_s_fun_SV, log_s_fun_obs, \
    f_fun_SV, f_fun_obs):
    """
    3DVAR

    3DVAR data assimilation technique for use in a general model, which allows for 
    Johnson distributed observations and background state variables

    #### Input
    - `init_guess`      ->  Initial guess for the model analysis, vector of size n_SV
    - `t_obs`           ->  Time values of observations, vector of size n_t_obs
    - `n_t_mod`         ->  Number of time steps to use in each model run
    - `y`               ->  Observations, array of size n_obs x n_t_obs
    - `phi`             ->  Observation operator, function of the form y = h(x)
    - `dphi`            ->  Gradient of the observation operator, 
                            function of the form y = dh/dx(x)
    - `B`               ->  Background error covariance matrix, array of size n_SV x n_SV
    - `R`               ->  Observation error covariance matrix, array of size n_obs x n_obs
    - `model`           ->  Model to use in the analysis, function of the form
                                x, M = model(t,x_0),
                            where t contains the time values where the model is evaluated
                            and x_0 is the initial condition;
                            the output x are the state variables for all time values
    - `t_fun_SV`        ->  Johnson transform for the state variables
    - `t_fun_obs`       ->  Johnson transform for the observations
    - `inv_t_fun_SV`    ->  Inverse Johnson transform for the state variables
    - `log_s_fun_SV`    ->  Log of the derivative of the transform, function 
                                S = log dT/dx(T^-1(X))
    - `f_fun_SV`        ->  Derivative of log_s_fun_SV, function 
                                F = d[log S(X)]/dX
    - `log_s_fun_obs`   ->  Same as log_s_fun_SV, but for observations
    - `f_fun_obs`       ->  Same as f_fun_SV, but for observations

    #### Output
    - `x_a`         ->  Analysis state, array of size n_SV x n_t_obs
    - `x_b`         ->  Background state, array of size n_SV x n_t
    - `t_true`      ->  Total time for background, vector of size n_t
    """
    
    # Save lengths of arrays
    n_SV = init_guess.size      # number of state variables
    n_t_obs = t_obs.size        # number of observations
    n_t = n_t_obs * n_t_mod + 1 # number of total time steps

    # Initialize background and analysis states
    x_b = np.empty((n_SV,n_t))
    x_a = np.empty((n_SV,n_t_obs))

    # At t=0, background and analysis states are set to the initial guess
    x_b[:,0] = init_guess
    x_a[:,0] = init_guess

    # Total time evolution for model
    dt_obs = t_obs[1] - t_obs[0] # Assuming evenly spaced observations
    t_true = np.linspace(t_obs[0],t_obs[-1] + dt_obs, n_t) # Assuming one prediction window

    # Invert observation error covariance matrix
    Rinv = inv(R)
    Binv = inv(B)

    modelFail = False
    # Loop over all observations
    for ii in range(n_t_obs - 1):
        # Calculated states are on model time, and not observation time
        tt = ii * n_t_mod       # Index for model time at observation point
        tt_next = tt + n_t_mod  # Index for model time at next observation point
        
        # Run the model for the analysis forecast
        sim_time = t_true[tt:tt_next+1]
        x_a_model = model(sim_time, x_a[:,ii]) # Model run for analysis state

        if np.any(np.isnan(x_a_model)):
            print("Warning: fail in model run, skipping this assimilation step!")
            print("    x_a_init = "+str(x_a[:,ii]))

            # If model fail, something went wrong in previous analysis
            x_a[:,ii] = np.nan*np.empty(n_SV)
            x_b[:,tt:tt_next+1] = np.nan*np.empty_like(sim_time)

            # Restart DA from initial guess next step
            x_a[:,ii+1] = init_guess
            modelFail = True
            continue

        # If previous 3DVAR was unsuccessful, set x_a to nan
        if ii > 0 and ((not res.success) or modelFail):
            x_a[:,ii] = np.nan*np.empty(n_SV)
            modelFail = False

        # Save model run in background
        x_b[:,tt+1:tt_next+1] = x_a_model[:,1:]

        # Transform background and observations
        try:
            x_b_mix = t_fun_SV(x_a_model[:,-1])
        except RuntimeWarning as message:
            print(message)
            print("Background out of bounds: "+str(x_a_model[:,-1]))
            x_a[:,ii+1] = x_a_model[:,-1]
            modelFail = True
            continue
        try:
            y_mix   = t_fun_obs(y[:,ii+1])
        except RuntimeWarning as message:
            print(message)
            print("Observation out of bounds: "+str(y[:,ii+1]))
            x_a[:,ii+1] = x_a_model[:,-1]
            modelFail = True
            continue

        tol = 1e-5

        res = minimize( \
            cost_3D, \
            x_b_mix, \
            args = (x_b_mix,y_mix,phi,dphi,Binv,Rinv, \
                log_s_fun_SV,log_s_fun_obs, \
                f_fun_SV,f_fun_obs), \
            jac = True, \
            tol = tol \
        )

        if not res.success:
            print('Minimum of the cost function was not found at t = '\
                    +str(t_obs[ii+1])+'.')

        # Set analysis state from solution
        x_a[:,ii+1] = inv_t_fun_SV(res.x)

    # Forecasting step
    ii += 1
    tt = ii * n_t_mod       # Index for model time at observation point
    tt_next = tt + n_t_mod  # Index for model time at next observation point
    
    # Run the model for the analysis
    sim_time = t_true[tt:tt_next+1]
    x_a_model = model(sim_time, x_a[:,ii]) # Model run for analysis state
    x_b[:,tt+1:tt_next+1] = x_a_model[:,1:]

    if np.any(np.isnan(x_a_model)):
        print("Warning: fail in final model run!")
        x_b[:,tt+1:tt_next+1] = np.nan*np.empty(sim_time.size-1)
        
    return x_a, x_b, t_true


def cost_3D(x, x_b, y, phi, dphi, Binv, Rinv, \
    log_s_fun_SV,log_s_fun_obs, \
    f_fun_SV,f_fun_obs):
    """
    Cost function for the 3DVAR assimilation step, which needs to be minimized
    to obtain the optimal analysis state from the background and observations

    The cost function is written in terms of the transformed variables
    
    #### Input
    - `x`               ->  State to be optimized, vector of size n_SV
    - `x_b`             ->  Background state, vector of size n_SV
    - `y`               ->  Observations, vector of size n_obs
    - `phi`             ->  Observation operator, function of the form y = h(x)
    - `dphi`            ->  Gradient of the observation operator, 
                            function of the form y = dh/dx(x)
    - `Binv`            ->  Inverse of the background error covariance matrix, 
                            array of size n_SV x n_SV
    - `Rinv`            ->  Inverse of the observation error covariance matrix, 
                            array of size n_obs x n_obs
    - `log_s_fun_SV`    ->  Log of the derivative of the transform, function 
                                S = log dT/dx(T^-1(X))
    - `f_fun_SV`        ->  Derivative of log_s_fun_SV, function 
                                F = d[log S(X)]/dX
    - `log_s_fun_obs`   ->  Same as log_s_fun_SV, but for observations
    - `f_fun_obs`       ->  Same as f_fun_SV, but for observations

    #### Output
    - `J`               ->  Cost function for 3DVAR
    - `grad`            ->  Gradient of the cost function
    """

    if np.any(x - x_b<-500.):
        return np.inf, np.full(x.shape,np.inf)
    # Background cost
    J_bg = (x - x_b) @ Binv @ (x - x_b)/2 - np.sum(log_s_fun_SV(x - x_b)) 

    # Observational cost
    h_x= phi(x)
    J_obs = (y - h_x) @ Rinv @ (y - h_x)/2 - np.sum(log_s_fun_obs(y - h_x)) 
    
    # Background gradient
    gradJ_bg = Binv @ (x - x_b) - (f_fun_SV(x - x_b)) 
    
    # Observational gradient
    dh_x = dphi(x)
    gradJ_obs = - dh_x.T @ Rinv @ (y - h_x) + (f_fun_obs(y - h_x) @ dh_x)
        
    return J_bg + J_obs, gradJ_bg + gradJ_obs
